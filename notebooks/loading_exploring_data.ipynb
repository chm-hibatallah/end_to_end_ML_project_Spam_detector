{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90b0ff50",
   "metadata": {},
   "source": [
    "# EDA  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0864b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7840fca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('C:\\\\Users\\\\dell\\\\Documents\\\\sms-spam-detection\\\\data\\\\spam.csv', encoding='latin-1')\n",
    "df = df[['v1', 'v2']]  # Keep only label and text columns\n",
    "df.columns = ['label', 'message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14ef65cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n",
      "Dataset shape: (5572, 2)\n"
     ]
    }
   ],
   "source": [
    "# Explore\n",
    "print(df.head())\n",
    "print(df['label'].value_counts())\n",
    "print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db200d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(403)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90bf3eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['char_len'] = df['message'].str.len()\n",
    "df['word_count'] = df['message'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef42d2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">char_len</th>\n",
       "      <th colspan=\"8\" halign=\"left\">word_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825.0</td>\n",
       "      <td>71.023627</td>\n",
       "      <td>58.016023</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>910.0</td>\n",
       "      <td>4825.0</td>\n",
       "      <td>14.200622</td>\n",
       "      <td>11.424511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>171.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747.0</td>\n",
       "      <td>138.866131</td>\n",
       "      <td>29.183082</td>\n",
       "      <td>13.0</td>\n",
       "      <td>132.5</td>\n",
       "      <td>149.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>747.0</td>\n",
       "      <td>23.851406</td>\n",
       "      <td>5.811898</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      char_len                                                           \\\n",
       "         count        mean        std   min    25%    50%    75%    max   \n",
       "label                                                                     \n",
       "ham     4825.0   71.023627  58.016023   2.0   33.0   52.0   92.0  910.0   \n",
       "spam     747.0  138.866131  29.183082  13.0  132.5  149.0  157.0  224.0   \n",
       "\n",
       "      word_count                                                      \n",
       "           count       mean        std  min   25%   50%   75%    max  \n",
       "label                                                                 \n",
       "ham       4825.0  14.200622  11.424511  1.0   7.0  11.0  19.0  171.0  \n",
       "spam       747.0  23.851406   5.811898  2.0  22.0  25.0  28.0   35.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label')[['char_len', 'word_count']].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44333eff",
   "metadata": {},
   "source": [
    "Spam messages have higher average length and word count compared to ham messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b13d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68e6addf",
   "metadata": {},
   "source": [
    "# data preprpcessing \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2834f09",
   "metadata": {},
   "source": [
    "## Clean text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d99b7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if needed\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "df['cleaned_message'] = df['message'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2a1c59",
   "metadata": {},
   "source": [
    "## Encode labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "373e5c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to binary (spam=1, ham=0)\n",
    "df['label_encoded'] = df['label'].map({'ham': 0, 'spam': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe17da0",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aa0050",
   "metadata": {},
   "source": [
    "## Text vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b0cb15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Bag of Words\n",
    "count_vectorizer = CountVectorizer(max_features=5000)\n",
    "X_count = count_vectorizer.fit_transform(df['cleaned_message'])\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e42ddb4",
   "metadata": {},
   "source": [
    "## Create additional features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b484c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length features\n",
    "df['message_length'] = df['message'].apply(len)\n",
    "df['word_count'] = df['message'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Check for spam keywords\n",
    "spam_keywords = ['free', 'win', 'winner', 'click', 'urgent', 'cash', 'prize']\n",
    "for keyword in spam_keywords:\n",
    "    df[f'contains_{keyword}'] = df['message'].str.contains(keyword, case=False).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee963a4",
   "metadata": {},
   "source": [
    "# Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41d12260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Using TF-IDF features\n",
    "X = X_tfidf\n",
    "y = df['label_encoded']\n",
    "\n",
    "# Split data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188ae842",
   "metadata": {},
   "source": [
    "# Model Building & Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdab21d",
   "metadata": {},
   "source": [
    "## Implement baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b440b2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "models = {\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'SVM': SVC(kernel='linear', probability=True),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cfc556",
   "metadata": {},
   "source": [
    "## Train all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acfbe139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB trained successfully\n",
      "LogisticRegression trained successfully\n",
      "SVM trained successfully\n",
      "RandomForest trained successfully\n"
     ]
    }
   ],
   "source": [
    "trained_models = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[name] = model\n",
    "    print(f\"{name} trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffec9446",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb84f08",
   "metadata": {},
   "source": [
    "## Create evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed38edc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "    }\n",
    "    \n",
    "    if y_proba is not None:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return metrics, cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e546c",
   "metadata": {},
   "source": [
    "## Compare all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d9a2f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MultinomialNB:\n",
      "  accuracy: 0.963\n",
      "  precision: 1.000\n",
      "  recall: 0.725\n",
      "  f1: 0.840\n",
      "  roc_auc: 0.976\n",
      "\n",
      "LogisticRegression:\n",
      "  accuracy: 0.967\n",
      "  precision: 0.991\n",
      "  recall: 0.758\n",
      "  f1: 0.859\n",
      "  roc_auc: 0.987\n",
      "\n",
      "SVM:\n",
      "  accuracy: 0.984\n",
      "  precision: 0.992\n",
      "  recall: 0.886\n",
      "  f1: 0.936\n",
      "  roc_auc: 0.985\n",
      "\n",
      "RandomForest:\n",
      "  accuracy: 0.970\n",
      "  precision: 1.000\n",
      "  recall: 0.779\n",
      "  f1: 0.875\n",
      "  roc_auc: 0.992\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for name, model in trained_models.items():\n",
    "    metrics, cm = evaluate_model(model, X_test, y_test)\n",
    "    results[name] = metrics\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242d160e",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b488031c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Best score: 0.905\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Example: Tune Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1fd051",
   "metadata": {},
   "source": [
    "# Model Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4d7975",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4fecca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 features indicating spam:\n",
      "      feature  coefficient\n",
      "4580      txt     4.776293\n",
      "474      call     4.311929\n",
      "1244     free     3.524839\n",
      "4242     stop     3.376621\n",
      "4486       to     3.291434\n",
      "4397     text     3.259726\n",
      "607     claim     3.108432\n",
      "3816    reply     2.979442\n",
      "1265     from     2.749723\n",
      "2955   mobile     2.715182\n",
      "4969     your     2.547711\n",
      "3420       or     2.428619\n",
      "4823      win     2.414735\n",
      "3992  service     2.391343\n",
      "4859      won     2.296210\n",
      "3648    prize     2.215809\n",
      "3326      now     2.099242\n",
      "570      chat     2.059531\n",
      "4636   urgent     1.986166\n",
      "1222      for     1.956557\n"
     ]
    }
   ],
   "source": [
    "# For Logistic Regression\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "coefs = trained_models['LogisticRegression'].coef_[0]\n",
    "\n",
    "# Get top 20 features for spam\n",
    "top_spam_features = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': coefs\n",
    "}).sort_values('coefficient', ascending=False).head(20)\n",
    "\n",
    "print(\"Top 20 features indicating spam:\")\n",
    "print(top_spam_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cecbb55",
   "metadata": {},
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "318e5fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of misclassified samples: 37\n"
     ]
    }
   ],
   "source": [
    "# Get misclassified examples\n",
    "model = trained_models['LogisticRegression']\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "misclassified_indices = np.where(y_pred != y_test)[0]\n",
    "misclassified_samples = df.iloc[misclassified_indices]\n",
    "\n",
    "print(f\"Number of misclassified samples: {len(misclassified_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a876e3",
   "metadata": {},
   "source": [
    "# Model Deployment Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19e581e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b96f8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
